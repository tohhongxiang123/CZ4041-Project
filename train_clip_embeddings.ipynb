{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import albumentations\n",
    "import cv2\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import clip\n",
    "from PIL import Image\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_IMAGES_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "OUTPUT_DIR = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data: pd.DataFrame, num_splits = 5):\n",
    "    data[\"fold\"] = -1\n",
    "    num_bins = int(np.floor(1 + np.log2(len(data)))) # sturge's rule\n",
    "\n",
    "    data.loc[:, \"bins\"] = pd.cut(data[\"Pawpularity\"], bins=num_bins, labels=False)\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold_index, (train_idx, val_idx) in enumerate(skf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[val_idx, 'fold'] = fold_index\n",
    "\n",
    "    data = data.drop('bins', axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = [\n",
    "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
    "]\n",
    "\n",
    "class PetFinderClipDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, dir: str, augmentations: albumentations.Compose):\n",
    "        self.ids = df[\"Id\"].values\n",
    "        if \"Pawpularity\" in df.keys():\n",
    "            self.targets = df[\"Pawpularity\"].values\n",
    "        else:\n",
    "            self.targets = [-1] * len(df)\n",
    "        self.dense_features = df[dense_features].values\n",
    "\n",
    "        image_paths = [os.path.join(dir, f\"{x}.jpg\") for x in df[\"Id\"].values]\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image_id = self.ids[item]\n",
    "\n",
    "        image = Image.open(os.path.join(self.image_paths[item]))\n",
    "        if self.augmentations is not None:\n",
    "            image = self.augmentations(image)\n",
    "        \n",
    "        features = self.dense_features[item, :]\n",
    "        targets = self.targets[item]\n",
    "        \n",
    "        return image_id, torch.tensor(features, dtype=torch.float), image, torch.tensor(targets, dtype=torch.float)\n",
    "    \n",
    "class PetFinderClipDataModule(LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 df_train=None, df_val=None, df_test=None, \n",
    "                 train_images_dir=None, val_images_dir=None, test_images_dir=None, \n",
    "                 train_augmentations=None, val_augmentations=None, test_augmentations=None, \n",
    "                 batch_size=64\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "        self.df_test = df_test\n",
    "\n",
    "        self.train_images_dir = train_images_dir\n",
    "        self.val_images_dir = val_images_dir\n",
    "        self.test_images_dir = test_images_dir\n",
    "\n",
    "        self.train_augmentations = train_augmentations\n",
    "        self.val_augmentations = val_augmentations\n",
    "        self.test_augmentations = test_augmentations\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PetFinderClipDataset(self.df_train, self.train_images_dir, self.train_augmentations), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PetFinderClipDataset(self.df_val, self.val_images_dir, self.val_augmentations), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(PetFinderClipDataset(self.df_test, self.test_images_dir, self.test_augmentations), batch_size=self.batch_size, shuffle=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 338M/338M [00:37<00:00, 9.43MiB/s]\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "\n",
    "df_train, df_val = train_test_split(df_train, test_size=0.2, random_state=SEED, shuffle=True, stratify=df_train['Pawpularity'])\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "df_val = df_val.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(dataloader):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image_ids, features, images, labels in tqdm(dataloader):\n",
    "            features = model.encode_image(images.to(device))\n",
    "\n",
    "            all_features.append(features)\n",
    "            all_labels.append(labels)\n",
    "\n",
    "    return torch.cat(all_features).cpu().numpy(), torch.cat(all_labels).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamodule = PetFinderClipDataModule(\n",
    "    df_train=df_train, train_images_dir=TRAIN_IMAGES_DIR, train_augmentations=preprocess,\n",
    "    df_val=df_val, val_images_dir=TRAIN_IMAGES_DIR, val_augmentations=preprocess,\n",
    "    batch_size=64\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 124/124 [01:36<00:00,  1.29it/s]\n"
     ]
    }
   ],
   "source": [
    "train_features, train_labels = get_features(datamodule.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7929, 512), (7929,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape, train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svr&#x27;, SVR(C=10, epsilon=0.2))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;standardscaler&#x27;, StandardScaler()),\n",
       "                (&#x27;svr&#x27;, SVR(C=10, epsilon=0.2))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(C=10, epsilon=0.2)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svr', SVR(C=10, epsilon=0.2))])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = make_pipeline(StandardScaler(), SVR(C=1, epsilon=0.2, kernel='rbf'))\n",
    "clf.fit(train_features, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(clf, os.path.join(OUTPUT_DIR, 'svm_clip.joblib'))\n",
    "clf = load(os.path.join(OUTPUT_DIR, 'svm_clip.joblib'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31/31 [00:20<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "val_features, val_labels = get_features(datamodule.val_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_with_clip_from_dataframe(df: pd.DataFrame, clf, image_dir: str = TRAIN_IMAGES_DIR, val_features=None, val_labels=None):\n",
    "    if val_features is None or val_labels is None:\n",
    "        datamodule = PetFinderClipDataModule(df_val=df, val_image_dir=image_dir, val_augmentations=preprocess)\n",
    "        val_features, val_labels = get_features(datamodule.val_dataloader())\n",
    "\n",
    "    val_predictions = clf.predict(val_features)\n",
    "    df[\"Pawpularity_prediction\"] = val_predictions\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val_with_predictions = get_predictions_with_clip_from_dataframe(df_val, clf, val_features=val_features, val_labels=val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Subject Focus</th>\n",
       "      <th>Eyes</th>\n",
       "      <th>Face</th>\n",
       "      <th>Near</th>\n",
       "      <th>Action</th>\n",
       "      <th>Accessory</th>\n",
       "      <th>Group</th>\n",
       "      <th>Collage</th>\n",
       "      <th>Human</th>\n",
       "      <th>Occlusion</th>\n",
       "      <th>Info</th>\n",
       "      <th>Blur</th>\n",
       "      <th>Pawpularity</th>\n",
       "      <th>Pawpularity_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>763c0fbfd75593c2911a33603c28dc45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>41.059297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ba6faa385160bedd384aed93329779fe</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>41.438748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dcae448678ce2779659104d1f5970e92</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>24.552653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>eab6c63493b68e1ab3817a094622be6f</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>36.910476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>291320168a3fb0d981b4e5ef5c26a307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>48.283062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1978</th>\n",
       "      <td>5130a71b585694927eda397d93a41e4d</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>36.100432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1979</th>\n",
       "      <td>d77cbb32318b41770954ed1a73e35dcb</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>32.914243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>feefa7d5530149a0538313c9815a90ad</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>29.234506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1981</th>\n",
       "      <td>60e31d807e3743cdd09130e2cf7c0d50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>38.147585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1982</th>\n",
       "      <td>9f181a86641f3fc0131d27032a5041d4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>38.470318</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1983 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    Id  Subject Focus  Eyes  Face  Near  \\\n",
       "0     763c0fbfd75593c2911a33603c28dc45              0     0     0     0   \n",
       "1     ba6faa385160bedd384aed93329779fe              0     1     1     0   \n",
       "2     dcae448678ce2779659104d1f5970e92              0     1     1     1   \n",
       "3     eab6c63493b68e1ab3817a094622be6f              0     0     1     0   \n",
       "4     291320168a3fb0d981b4e5ef5c26a307              0     1     1     1   \n",
       "...                                ...            ...   ...   ...   ...   \n",
       "1978  5130a71b585694927eda397d93a41e4d              0     1     1     1   \n",
       "1979  d77cbb32318b41770954ed1a73e35dcb              0     1     1     1   \n",
       "1980  feefa7d5530149a0538313c9815a90ad              0     1     1     1   \n",
       "1981  60e31d807e3743cdd09130e2cf7c0d50              0     0     1     1   \n",
       "1982  9f181a86641f3fc0131d27032a5041d4              0     0     1     1   \n",
       "\n",
       "      Action  Accessory  Group  Collage  Human  Occlusion  Info  Blur  \\\n",
       "0          0          0      1        0      0          0     0     0   \n",
       "1          0          0      1        0      0          0     1     0   \n",
       "2          0          0      0        0      0          0     0     0   \n",
       "3          0          0      0        1      0          0     1     1   \n",
       "4          0          0      0        0      0          0     0     0   \n",
       "...      ...        ...    ...      ...    ...        ...   ...   ...   \n",
       "1978       0          0      0        0      0          0     0     0   \n",
       "1979       0          1      0        0      0          0     0     0   \n",
       "1980       0          0      0        0      0          0     0     0   \n",
       "1981       0          0      0        0      0          0     0     1   \n",
       "1982       0          0      0        0      0          0     0     0   \n",
       "\n",
       "      Pawpularity  Pawpularity_prediction  \n",
       "0              18               41.059297  \n",
       "1              52               41.438748  \n",
       "2              32               24.552653  \n",
       "3              35               36.910476  \n",
       "4              23               48.283062  \n",
       "...           ...                     ...  \n",
       "1978           34               36.100432  \n",
       "1979           48               32.914243  \n",
       "1980           20               29.234506  \n",
       "1981           30               38.147585  \n",
       "1982           41               38.470318  \n",
       "\n",
       "[1983 rows x 15 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val_with_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.474123624936215"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(df_val_with_predictions['Pawpularity'], df_val_with_predictions['Pawpularity_prediction'], squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, os.path.join(OUTPUT_DIR, 'model_checkpoints', 'clip.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Clip and save to zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\n",
      "  Cloning https://github.com/openai/CLIP.git to c:\\users\\q\\appdata\\local\\temp\\pip-req-build-to9c22xk\n",
      "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting ftfy (from clip==1.0)\n",
      "  Using cached ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "Collecting regex (from clip==1.0)\n",
      "  Using cached regex-2023.10.3-cp311-cp311-win_amd64.whl (269 kB)\n",
      "Collecting tqdm (from clip==1.0)\n",
      "  Using cached tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
      "Collecting torch (from clip==1.0)\n",
      "  Using cached torch-2.1.0-cp311-cp311-win_amd64.whl (192.3 MB)\n",
      "Collecting torchvision (from clip==1.0)\n",
      "  Using cached torchvision-0.16.0-cp311-cp311-win_amd64.whl (1.3 MB)\n",
      "Collecting wcwidth>=0.2.5 (from ftfy->clip==1.0)\n",
      "  Using cached wcwidth-0.2.9-py2.py3-none-any.whl (102 kB)\n",
      "Collecting filelock (from torch->clip==1.0)\n",
      "  Downloading filelock-3.13.1-py3-none-any.whl (11 kB)\n",
      "Collecting typing-extensions (from torch->clip==1.0)\n",
      "  Using cached typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
      "Collecting sympy (from torch->clip==1.0)\n",
      "  Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
      "Collecting networkx (from torch->clip==1.0)\n",
      "  Using cached networkx-3.2.1-py3-none-any.whl (1.6 MB)\n",
      "Collecting jinja2 (from torch->clip==1.0)\n",
      "  Using cached Jinja2-3.1.2-py3-none-any.whl (133 kB)\n",
      "Collecting fsspec (from torch->clip==1.0)\n",
      "  Using cached fsspec-2023.10.0-py3-none-any.whl (166 kB)\n",
      "Collecting numpy (from torchvision->clip==1.0)\n",
      "  Using cached numpy-1.26.1-cp311-cp311-win_amd64.whl (15.8 MB)\n",
      "Collecting requests (from torchvision->clip==1.0)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0 (from torchvision->clip==1.0)\n",
      "  Using cached Pillow-10.1.0-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Collecting colorama (from tqdm->clip==1.0)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch->clip==1.0)\n",
      "  Using cached MarkupSafe-2.1.3-cp311-cp311-win_amd64.whl (17 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->torchvision->clip==1.0)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "                                              0.0/99.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 99.9/99.9 kB 5.6 MB/s eta 0:00:00\n",
      "Collecting idna<4,>=2.5 (from requests->torchvision->clip==1.0)\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->torchvision->clip==1.0)\n",
      "  Using cached urllib3-2.0.7-py3-none-any.whl (124 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->torchvision->clip==1.0)\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting mpmath>=0.19 (from sympy->torch->clip==1.0)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\clip-1.0.zip\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\ftfy-6.1.1-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\regex-2023.10.3-cp311-cp311-win_amd64.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\torch-2.1.0-cp311-cp311-win_amd64.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\torchvision-0.16.0-cp311-cp311-win_amd64.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\tqdm-4.66.1-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\pillow-10.1.0-cp311-cp311-win_amd64.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\wcwidth-0.2.9-py2.py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\colorama-0.4.6-py2.py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\filelock-3.13.1-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\fsspec-2023.10.0-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\jinja2-3.1.2-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\networkx-3.2.1-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\numpy-1.26.1-cp311-cp311-win_amd64.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\requests-2.31.0-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\sympy-1.12-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\typing_extensions-4.8.0-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\certifi-2023.7.22-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\idna-3.4-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\markupsafe-2.1.3-cp311-cp311-win_amd64.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\mpmath-1.3.0-py3-none-any.whl\n",
      "Saved c:\\users\\q\\desktop\\programming\\ml\\clip\\urllib3-2.0.7-py3-none-any.whl\n",
      "Successfully downloaded clip ftfy regex torch torchvision tqdm pillow wcwidth colorama filelock fsspec jinja2 networkx numpy requests sympy typing-extensions certifi charset-normalizer idna MarkupSafe mpmath urllib3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git 'C:\\Users\\q\\AppData\\Local\\Temp\\pip-req-build-to9c22xk'\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip download git+https://github.com/openai/CLIP.git -d ./clip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from zipfile import ZipFile\n",
    "\n",
    "dirName = \"./clip\"\n",
    "zipName = \"packages.zip\"\n",
    "\n",
    "# Create a ZipFile Object\n",
    "with ZipFile(zipName, 'w') as zipObj:\n",
    "    # Iterate over all the files in directory\n",
    "    for folderName, subfolders, filenames in os.walk(dirName):\n",
    "        for filename in filenames:\n",
    "            if (filename != zipName):\n",
    "                # create complete filepath of file in directory\n",
    "                filePath = os.path.join(folderName, filename)\n",
    "                # Add file to zip\n",
    "                zipObj.write(filePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__path__',\n",
       " '__spec__',\n",
       " 'available_models',\n",
       " 'clip',\n",
       " 'load',\n",
       " 'model',\n",
       " 'simple_tokenizer',\n",
       " 'tokenize']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=224, interpolation=bicubic, max_size=None, antialias=warn)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    <function _convert_image_to_rgb at 0x000001D660264B80>\n",
       "    ToTensor()\n",
       "    Normalize(mean=(0.48145466, 0.4578275, 0.40821073), std=(0.26862954, 0.26130258, 0.27577711))\n",
       ")"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
