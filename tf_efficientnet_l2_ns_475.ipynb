{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n",
      "  warnings.warn(\"No audio backend is available.\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import albumentations\n",
    "import cv2\n",
    "import timm\n",
    "import torch.nn as nn\n",
    "from sklearn import metrics\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import callbacks\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_lightning import LightningDataModule\n",
    "\n",
    "import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2023\n",
    "\n",
    "DATA_DIR = 'data'\n",
    "\n",
    "TRAIN_IMAGES_DIR = os.path.join(DATA_DIR, 'train')\n",
    "TEST_IMAGES_DIR = os.path.join(DATA_DIR, 'test')\n",
    "\n",
    "OUTPUT_DIR = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_folds(data: pd.DataFrame, num_splits = 5):\n",
    "    data[\"fold\"] = -1\n",
    "    num_bins = int(np.floor(1 + np.log2(len(data)))) # sturge's rule\n",
    "\n",
    "    data.loc[:, \"bins\"] = pd.cut(data[\"Pawpularity\"], bins=num_bins, labels=False)\n",
    "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=SEED)\n",
    "\n",
    "    for fold_index, (train_idx, val_idx) in enumerate(skf.split(X=data, y=data.bins.values)):\n",
    "        data.loc[val_idx, 'fold'] = fold_index\n",
    "\n",
    "    data = data.drop('bins', axis=1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_features = [\n",
    "    'Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n",
    "    'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'\n",
    "]\n",
    "\n",
    "class PetFinderDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, dir: str, augmentations: albumentations.Compose):\n",
    "        self.ids = df[\"Id\"].values\n",
    "        if \"Pawpularity\" in df.keys():\n",
    "            self.targets = df[\"Pawpularity\"].values\n",
    "        else:\n",
    "            self.targets = [-1] * len(df)\n",
    "        self.dense_features = df[dense_features].values\n",
    "\n",
    "        image_paths = [os.path.join(dir, f\"{x}.jpg\") for x in df[\"Id\"].values]\n",
    "        self.image_paths = image_paths\n",
    "\n",
    "        self.augmentations = augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        image_id = self.ids[item]\n",
    "\n",
    "        image = cv2.imread(self.image_paths[item])\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        if self.augmentations is not None:\n",
    "            augmented = self.augmentations(image=image)\n",
    "            image = augmented[\"image\"]\n",
    "            \n",
    "        image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "        \n",
    "        features = self.dense_features[item, :]\n",
    "        targets = self.targets[item]\n",
    "        \n",
    "        return image_id, torch.tensor(features, dtype=torch.float), torch.tensor(image, dtype=torch.float), torch.tensor(targets, dtype=torch.float)\n",
    "    \n",
    "class PetFinderDataModule(LightningDataModule):\n",
    "    def __init__(self, \n",
    "                 df_train=None, df_val=None, df_test=None, \n",
    "                 train_images_dir=None, val_images_dir=None, test_images_dir=None, \n",
    "                 train_augmentations=None, val_augmentations=None, test_augmentations=None, \n",
    "                 batch_size=64\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.df_train = df_train\n",
    "        self.df_val = df_val\n",
    "        self.df_test = df_test\n",
    "\n",
    "        self.train_images_dir = train_images_dir\n",
    "        self.val_images_dir = val_images_dir\n",
    "        self.test_images_dir = test_images_dir\n",
    "\n",
    "        self.train_augmentations = train_augmentations\n",
    "        self.val_augmentations = val_augmentations\n",
    "        self.test_augmentations = test_augmentations\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(PetFinderDataset(self.df_train, self.train_images_dir, self.train_augmentations), batch_size=self.batch_size, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(PetFinderDataset(self.df_val, self.val_images_dir, self.val_augmentations), batch_size=self.batch_size, shuffle=False)\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(PetFinderDataset(self.df_test, self.test_images_dir, self.test_augmentations), batch_size=self.batch_size, shuffle=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PawpularityModel(pl.LightningModule):\n",
    "    def __init__(self, model_name=\"tf_efficientnet_b0_ns\", pretrained=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = timm.create_model(model_name=model_name, pretrained=pretrained, in_chans=3)\n",
    "        self.backbone.classifier = nn.Linear(self.backbone.classifier.in_features, 128)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(128 + 12, 1)\n",
    "\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "        self.validation_step_outputs = []\n",
    "        self.training_step_outputs = []\n",
    "\n",
    "    def forward(self, input, features):\n",
    "        x = self.backbone(input)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = torch.cat([x, features], dim=1)\n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def training_step(self, batch, batch_indexes):\n",
    "        loss, predictions, labels, rmse = self.step(batch, 'train')\n",
    "        self.training_step_outputs.append({ \"rmse\": rmse, \"loss\": loss })\n",
    "\n",
    "        return { 'loss': loss, 'predictions': predictions, 'labels': labels }\n",
    "\n",
    "    def validation_step(self, batch, batch_indexes):\n",
    "        loss, predictions, labels, rmse = self.step(batch, 'val')\n",
    "        self.validation_step_outputs.append({ \"rmse\": rmse, \"loss\": loss })\n",
    "        \n",
    "        return { 'loss': loss, 'predictions': predictions, 'labels': labels }\n",
    "\n",
    "    def step(self, batch, mode):\n",
    "        image_ids, features, images, labels = batch\n",
    "        labels = labels.float() / 100.0\n",
    "\n",
    "        logits = self.forward(images, features).squeeze(1)\n",
    "        loss = self.criterion(logits, labels) # using BCELoss to optimize models\n",
    "\n",
    "        predictions = logits.sigmoid().detach().cpu() * 100\n",
    "        labels = labels.detach().cpu() * 100\n",
    "        \n",
    "        rmse = mean_squared_error(predictions, labels, squared=False) # keeping track of RMSE as it is the competition metric\n",
    "        rmse = torch.tensor(rmse, dtype=torch.float32)\n",
    "\n",
    "        self.log(f'{mode}_loss', loss)\n",
    "        \n",
    "        return loss, predictions, labels, rmse\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        rsmes = [x[\"rmse\"] for x in self.training_step_outputs]\n",
    "        rsme = torch.stack(rsmes).mean()\n",
    "\n",
    "        self.log(f'train_rmse', rsme, prog_bar=True)\n",
    "\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        rsmes = [x[\"rmse\"] for x in self.validation_step_outputs]\n",
    "        rsme = torch.stack(rsmes).mean()\n",
    "\n",
    "        self.log(f'val_rmse', rsme, prog_bar=True)\n",
    "        \n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0 = 20, eta_min=1e-4)\n",
    "\n",
    "        return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_augmentations(image_size: int):\n",
    "    train_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(image_size, image_size, p=1),\n",
    "            albumentations.HueSaturationValue(\n",
    "                hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5\n",
    "            ),\n",
    "            albumentations.RandomBrightnessContrast(\n",
    "                brightness_limit=(-0.1, 0.1), contrast_limit=(-0.1, 0.1), p=0.5\n",
    "            ),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0,\n",
    "            ),\n",
    "        ],\n",
    "        p=1.0,\n",
    "    )\n",
    "\n",
    "    valid_aug = albumentations.Compose(\n",
    "        [\n",
    "            albumentations.Resize(image_size, image_size, p=1),\n",
    "            albumentations.Normalize(\n",
    "                mean=[0.485, 0.456, 0.406],\n",
    "                std=[0.229, 0.224, 0.225],\n",
    "                max_pixel_value=255.0,\n",
    "                p=1.0,\n",
    "            ),\n",
    "        ],\n",
    "        p=1.0,\n",
    "    )\n",
    "\n",
    "    return train_aug, valid_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\timm\\models\\_factory.py:114: UserWarning: Mapping deprecated model name tf_efficientnet_l2_ns_475 to current tf_efficientnet_l2.ns_jft_in1k_475.\n",
      "  model = create_fn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type              | Params\n",
      "------------------------------------------------\n",
      "0 | backbone  | EfficientNet      | 475 M \n",
      "1 | dropout   | Dropout           | 0     \n",
      "2 | out       | Linear            | 141   \n",
      "3 | criterion | BCEWithLogitsLoss | 0     \n",
      "------------------------------------------------\n",
      "475 M     Trainable params\n",
      "0         Non-trainable params\n",
      "475 M     Total params\n",
      "1,902.036 Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a9ae71ca8814f8ea4105f66d53e7096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "c:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:442: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92404442bc394f2691ff1d07f45c5933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 2.00 GiB total capacity; 8.42 GiB already allocated; 0 bytes free; 8.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m logger \u001b[39m=\u001b[39m TensorBoardLogger(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(OUTPUT_DIR, \u001b[39m\"\u001b[39m\u001b[39mlogs\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[0;32m     38\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(max_epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m, callbacks\u001b[39m=\u001b[39m[lr_monitor, loss_checkpoint, early_stopping], logger\u001b[39m=\u001b[39mlogger)\n\u001b[1;32m---> 39\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(model, datamodule\u001b[39m=\u001b[39;49mdata_module)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:532\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m    531\u001b[0m _verify_strategy_supports_compile(model, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy)\n\u001b[1;32m--> 532\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[0;32m    533\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[0;32m    534\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:43\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     42\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 43\u001b[0m     \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     45\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     46\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:571\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector\u001b[39m.\u001b[39mattach_data(\n\u001b[0;32m    562\u001b[0m     model, train_dataloaders\u001b[39m=\u001b[39mtrain_dataloaders, val_dataloaders\u001b[39m=\u001b[39mval_dataloaders, datamodule\u001b[39m=\u001b[39mdatamodule\n\u001b[0;32m    563\u001b[0m )\n\u001b[0;32m    565\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    566\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[0;32m    567\u001b[0m     ckpt_path,\n\u001b[0;32m    568\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    569\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    570\u001b[0m )\n\u001b[1;32m--> 571\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49mckpt_path)\n\u001b[0;32m    573\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[0;32m    574\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:980\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_signal_connector\u001b[39m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    977\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    978\u001b[0m \u001b[39m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    979\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 980\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[0;32m    982\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    983\u001b[0m \u001b[39m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    984\u001b[0m \u001b[39m# ----------------------------\u001b[39;00m\n\u001b[0;32m    985\u001b[0m log\u001b[39m.\u001b[39mdebug(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1023\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1021\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_sanity_check()\n\u001b[0;32m   1022\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[1;32m-> 1023\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[0;32m   1024\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1025\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnexpected state \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:202\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start()\n\u001b[1;32m--> 202\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance()\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    204\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:355\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_fetcher\u001b[39m.\u001b[39msetup(combined_loader)\n\u001b[0;32m    354\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_epoch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:133\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    131\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone:\n\u001b[0;32m    132\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 133\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(data_fetcher)\n\u001b[0;32m    134\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[0;32m    135\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\training_epoch_loop.py:219\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[1;34m(self, data_fetcher)\u001b[0m\n\u001b[0;32m    216\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39m\"\u001b[39m\u001b[39mrun_training_batch\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    217\u001b[0m     \u001b[39mif\u001b[39;00m trainer\u001b[39m.\u001b[39mlightning_module\u001b[39m.\u001b[39mautomatic_optimization:\n\u001b[0;32m    218\u001b[0m         \u001b[39m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mautomatic_optimization\u001b[39m.\u001b[39;49mrun(trainer\u001b[39m.\u001b[39;49moptimizers[\u001b[39m0\u001b[39;49m], kwargs)\n\u001b[0;32m    220\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    221\u001b[0m         batch_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmanual_optimization\u001b[39m.\u001b[39mrun(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:188\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[1;34m(self, optimizer, kwargs)\u001b[0m\n\u001b[0;32m    181\u001b[0m         closure()\n\u001b[0;32m    183\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[39m# BACKWARD PASS\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[39m# ------------------------------\u001b[39;00m\n\u001b[0;32m    186\u001b[0m \u001b[39m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[0;32m    187\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer_step(kwargs\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mbatch_idx\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m0\u001b[39;49m), closure)\n\u001b[0;32m    190\u001b[0m result \u001b[39m=\u001b[39m closure\u001b[39m.\u001b[39mconsume_result()\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m result\u001b[39m.\u001b[39mloss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:266\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[1;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_ready()\n\u001b[0;32m    265\u001b[0m \u001b[39m# model hook\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m call\u001b[39m.\u001b[39;49m_call_lightning_module_hook(\n\u001b[0;32m    267\u001b[0m     trainer,\n\u001b[0;32m    268\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39moptimizer_step\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    269\u001b[0m     trainer\u001b[39m.\u001b[39;49mcurrent_epoch,\n\u001b[0;32m    270\u001b[0m     batch_idx,\n\u001b[0;32m    271\u001b[0m     optimizer,\n\u001b[0;32m    272\u001b[0m     train_step_and_backward_closure,\n\u001b[0;32m    273\u001b[0m )\n\u001b[0;32m    275\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m should_accumulate:\n\u001b[0;32m    276\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptim_progress\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mstep\u001b[39m.\u001b[39mincrement_completed()\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:146\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m    143\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m hook_name\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[LightningModule]\u001b[39m\u001b[39m{\u001b[39;00mpl_module\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 146\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    148\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    149\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\core\\module.py:1276\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[1;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[0;32m   1238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39moptimizer_step\u001b[39m(\n\u001b[0;32m   1239\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   1240\u001b[0m     epoch: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1243\u001b[0m     optimizer_closure: Optional[Callable[[], Any]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1244\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1245\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer`\u001b[39;00m\n\u001b[0;32m   1246\u001b[0m \u001b[39m    calls the optimizer.\u001b[39;00m\n\u001b[0;32m   1247\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1274\u001b[0m \u001b[39m                    pg[\"lr\"] = lr_scale * self.learning_rate\u001b[39;00m\n\u001b[0;32m   1275\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1276\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49moptimizer_closure)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\core\\optimizer.py:161\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[1;34m(self, closure, **kwargs)\u001b[0m\n\u001b[0;32m    158\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\u001b[39m\"\u001b[39m\u001b[39mWhen `optimizer.step(closure)` is called, the closure should be callable\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    160\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_strategy \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 161\u001b[0m step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_strategy\u001b[39m.\u001b[39;49moptimizer_step(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimizer, closure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    163\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_on_after_step()\n\u001b[0;32m    165\u001b[0m \u001b[39mreturn\u001b[39;00m step_output\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:231\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[1;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[0;32m    229\u001b[0m \u001b[39m# TODO(fabric): remove assertion once strategy's optimizer_step typing is fixed\u001b[39;00m\n\u001b[0;32m    230\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(model, pl\u001b[39m.\u001b[39mLightningModule)\n\u001b[1;32m--> 231\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprecision_plugin\u001b[39m.\u001b[39;49moptimizer_step(optimizer, model\u001b[39m=\u001b[39;49mmodel, closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:116\u001b[0m, in \u001b[0;36mPrecisionPlugin.optimizer_step\u001b[1;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Hook to run the optimizer step.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_wrap_closure, model, optimizer, closure)\n\u001b[1;32m--> 116\u001b[0m \u001b[39mreturn\u001b[39;00m optimizer\u001b[39m.\u001b[39;49mstep(closure\u001b[39m=\u001b[39;49mclosure, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\optim\\adamw.py:148\u001b[0m, in \u001b[0;36mAdamW.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m closure \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    147\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[1;32m--> 148\u001b[0m         loss \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    150\u001b[0m \u001b[39mfor\u001b[39;00m group \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparam_groups:\n\u001b[0;32m    151\u001b[0m     params_with_grad \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\plugins\\precision\\precision_plugin.py:103\u001b[0m, in \u001b[0;36mPrecisionPlugin._wrap_closure\u001b[1;34m(self, model, optimizer, closure)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_wrap_closure\u001b[39m(\n\u001b[0;32m     92\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m     93\u001b[0m     model: \u001b[39m\"\u001b[39m\u001b[39mpl.LightningModule\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m     optimizer: Optimizer,\n\u001b[0;32m     95\u001b[0m     closure: Callable[[], Any],\n\u001b[0;32m     96\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    ``on_before_optimizer_step`` hook is called.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \n\u001b[0;32m    100\u001b[0m \u001b[39m    The closure (generally) runs ``backward`` so this allows inspecting gradients in this hook. This structure is\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[39m    consistent with the ``PrecisionPlugin`` subclasses that cannot pass ``optimizer.step(closure)`` directly.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     closure_result \u001b[39m=\u001b[39m closure()\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_after_closure(model, optimizer)\n\u001b[0;32m    105\u001b[0m     \u001b[39mreturn\u001b[39;00m closure_result\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:142\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Optional[Tensor]:\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclosure(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    143\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_result\u001b[39m.\u001b[39mloss\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:128\u001b[0m, in \u001b[0;36mClosure.closure\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39menable_grad()\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mclosure\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ClosureResult:\n\u001b[1;32m--> 128\u001b[0m     step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_step_fn()\n\u001b[0;32m    130\u001b[0m     \u001b[39mif\u001b[39;00m step_output\u001b[39m.\u001b[39mclosure_loss \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    131\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwarning_cache\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39m`training_step` returned `None`. If this was on purpose, ignore this warning...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\loops\\optimization\\automatic.py:315\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m trainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\n\u001b[0;32m    314\u001b[0m \u001b[39m# manually capture logged metrics\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m training_step_output \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49m_call_strategy_hook(trainer, \u001b[39m\"\u001b[39;49m\u001b[39mtraining_step\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[0;32m    316\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mpost_training_step()\n\u001b[0;32m    318\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_result_cls\u001b[39m.\u001b[39mfrom_training_step_output(training_step_output, trainer\u001b[39m.\u001b[39maccumulate_grad_batches)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:294\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[39mwith\u001b[39;00m trainer\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Strategy]\u001b[39m\u001b[39m{\u001b[39;00mtrainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 294\u001b[0m     output \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    296\u001b[0m \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    297\u001b[0m pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py:380\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprecision_plugin\u001b[39m.\u001b[39mtrain_step_context():\n\u001b[0;32m    379\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel, TrainingStep)\n\u001b[1;32m--> 380\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtraining_step(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[1;32mIn[5], line 25\u001b[0m, in \u001b[0;36mPawpularityModel.training_step\u001b[1;34m(self, batch, batch_indexes)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtraining_step\u001b[39m(\u001b[39mself\u001b[39m, batch, batch_indexes):\n\u001b[1;32m---> 25\u001b[0m     loss, predictions, labels, rmse \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep(batch, \u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m     26\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_step_outputs\u001b[39m.\u001b[39mappend({ \u001b[39m\"\u001b[39m\u001b[39mrmse\u001b[39m\u001b[39m\"\u001b[39m: rmse, \u001b[39m\"\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m\"\u001b[39m: loss })\n\u001b[0;32m     28\u001b[0m     \u001b[39mreturn\u001b[39;00m { \u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m: loss, \u001b[39m'\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m'\u001b[39m: predictions, \u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m: labels }\n",
      "Cell \u001b[1;32mIn[5], line 40\u001b[0m, in \u001b[0;36mPawpularityModel.step\u001b[1;34m(self, batch, mode)\u001b[0m\n\u001b[0;32m     37\u001b[0m image_ids, features, images, labels \u001b[39m=\u001b[39m batch\n\u001b[0;32m     38\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mfloat() \u001b[39m/\u001b[39m \u001b[39m100.0\u001b[39m\n\u001b[1;32m---> 40\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(images, features)\u001b[39m.\u001b[39msqueeze(\u001b[39m1\u001b[39m)\n\u001b[0;32m     41\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcriterion(logits, labels) \u001b[39m# using BCELoss to optimize models\u001b[39;00m\n\u001b[0;32m     43\u001b[0m predictions \u001b[39m=\u001b[39m logits\u001b[39m.\u001b[39msigmoid()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu() \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mPawpularityModel.forward\u001b[1;34m(self, input, features)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, features):\n\u001b[1;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m     17\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(x)\n\u001b[0;32m     19\u001b[0m     x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([x, features], dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\timm\\models\\efficientnet.py:179\u001b[0m, in \u001b[0;36mEfficientNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m--> 179\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_features(x)\n\u001b[0;32m    180\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_head(x)\n\u001b[0;32m    181\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\timm\\models\\efficientnet.py:167\u001b[0m, in \u001b[0;36mEfficientNet.forward_features\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    165\u001b[0m     x \u001b[39m=\u001b[39m checkpoint_seq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks, x, flatten\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 167\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblocks(x)\n\u001b[0;32m    168\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_head(x)\n\u001b[0;32m    169\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\timm\\models\\_efficientnet_blocks.py:182\u001b[0m, in \u001b[0;36mInvertedResidual.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    180\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv_pw(x)\n\u001b[0;32m    181\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(x)\n\u001b[1;32m--> 182\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv_dw(x)\n\u001b[0;32m    183\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn2(x)\n\u001b[0;32m    184\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mse(x)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\q\\Desktop\\Programming\\ML\\venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 136.00 MiB (GPU 0; 2.00 GiB total capacity; 8.42 GiB already allocated; 0 bytes free; 8.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "df = pd.read_csv(os.path.join(DATA_DIR, 'train.csv'))\n",
    "num_splits = 5\n",
    "df_5 = create_folds(df, num_splits=num_splits)\n",
    "\n",
    "model_name = \"tf_efficientnet_l2_ns_475\"\n",
    "\n",
    "image_size = 475\n",
    "train_aug, valid_aug = create_augmentations(image_size)\n",
    "\n",
    "for fold_index in range(num_splits):\n",
    "    df_train = df_5[df_5['fold'] != fold_index].reset_index(drop=True)\n",
    "    df_val = df_5[df_5['fold'] == fold_index].reset_index(drop=True)\n",
    "\n",
    "    data_module = PetFinderDataModule(\n",
    "        df_train=df_train, \n",
    "        df_val=df_val, \n",
    "        train_images_dir=TRAIN_IMAGES_DIR, \n",
    "        val_images_dir=TRAIN_IMAGES_DIR, \n",
    "        train_augmentations=train_aug,\n",
    "        val_augmentations=valid_aug,\n",
    "        batch_size=4\n",
    "    )\n",
    "\n",
    "    model = PawpularityModel(model_name=model_name, pretrained=True)\n",
    "\n",
    "    early_stopping = EarlyStopping(monitor=\"val_loss\")\n",
    "    lr_monitor = callbacks.LearningRateMonitor()\n",
    "    loss_checkpoint = callbacks.ModelCheckpoint(dirpath=os.path.join(OUTPUT_DIR, \"model_checkpoints\", model_name, f\"fold_{fold_index}\"), filename=\"best_loss\", monitor=\"val_loss\", save_top_k=1, mode=\"min\", save_last=False)\n",
    "\n",
    "    logger = TensorBoardLogger(os.path.join(OUTPUT_DIR, \"logs\"))\n",
    "\n",
    "    trainer = pl.Trainer(max_epochs=20, callbacks=[lr_monitor, loss_checkpoint, early_stopping], logger=logger)\n",
    "    trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
